{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1.3: Optimization Basics\n",
    "## Gradient Descent Implementation\n",
    "\n",
    "**Module**: Mathematical Foundations  \n",
    "**Project ID**: 1.3  \n",
    "**Estimated Time**: 75 minutes  \n",
    "**Prerequisites**: Projects 1.1-1.2, Basic calculus concepts  \n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this project, you will be able to:\n",
    "- [ ] Understand derivatives and gradients\n",
    "- [ ] Implement gradient descent from scratch\n",
    "- [ ] Visualize optimization landscapes\n",
    "- [ ] Tune learning rates for convergence\n",
    "\n",
    "### Mathematical Concepts Covered\n",
    "- **Derivatives**: Rate of change, slope of tangent\n",
    "- **Gradients**: Direction of steepest ascent\n",
    "- **Gradient Descent**: Iterative optimization algorithm\n",
    "- **Learning Rate**: Step size control\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úì Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Optimization is at the heart of machine learning. Every time you train a model, you're solving an optimization problem: finding the parameters that minimize error.\n",
    "\n",
    "**Key Applications**:\n",
    "- Training neural networks\n",
    "- Fitting regression models\n",
    "- Hyperparameter tuning\n",
    "- Resource allocation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theoretical Background\n",
    "\n",
    "### 2.1 Derivatives\n",
    "\n",
    "The derivative $f'(x)$ represents the rate of change of function $f$ at point $x$:\n",
    "\\[\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "\\]\n",
    "\n",
    "### 2.2 Gradient Descent Algorithm\n",
    "\n",
    "To minimize function $f(x)$:\n",
    "\\[\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\alpha \\nabla f(x_{\\text{old}})\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (step size)\n",
    "- $\\nabla f$ = gradient (direction of steepest ascent)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Numerical Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    Compute numerical derivative using finite differences.\n",
    "    \n",
    "    f'(x) ‚âà (f(x+h) - f(x-h)) / (2h)\n",
    "    \"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Test on f(x) = x^2\n",
    "f = lambda x: x**2\n",
    "x_test = 3.0\n",
    "\n",
    "numerical = numerical_derivative(f, x_test)\n",
    "analytical = 2 * x_test  # f'(x) = 2x\n",
    "\n",
    "print(f\"f(x) = x¬≤ at x = {x_test}\")\n",
    "print(f\"Numerical derivative: {numerical:.6f}\")\n",
    "print(f\"Analytical derivative: {analytical:.6f}\")\n",
    "print(f\"Error: {abs(numerical - analytical):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, x0, learning_rate=0.1, n_iterations=100, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Gradient descent optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    f : callable\n",
    "        Function to minimize\n",
    "    x0 : float\n",
    "        Initial guess\n",
    "    learning_rate : float\n",
    "        Step size\n",
    "    n_iterations : int\n",
    "        Maximum iterations\n",
    "    tolerance : float\n",
    "        Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    x_opt : float\n",
    "        Optimized x value\n",
    "    history : list\n",
    "        History of x values\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Compute gradient\n",
    "        grad = numerical_derivative(f, x)\n",
    "        \n",
    "        # Update\n",
    "        x_new = x - learning_rate * grad\n",
    "        history.append(x_new)\n",
    "        \n",
    "        # Check convergence\n",
    "        if abs(x_new - x) < tolerance:\n",
    "            print(f\"Converged after {i+1} iterations\")\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Test: Minimize f(x) = (x-3)¬≤ + 5\n",
    "# Minimum should be at x = 3 with f(3) = 5\n",
    "f = lambda x: (x - 3)**2 + 5\n",
    "\n",
    "x_opt, history = gradient_descent(f, x0=0.0, learning_rate=0.3)\n",
    "\n",
    "print(f\"\\nOptimization Results:\")\n",
    "print(f\"Initial x: 0.0, f(x): {f(0.0):.4f}\")\n",
    "print(f\"Optimized x: {x_opt:.6f}, f(x): {f(x_opt):.6f}\")\n",
    "print(f\"Expected: x = 3.0, f(x) = 5.0\")\n",
    "print(f\"Total iterations: {len(history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent\n",
    "x_range = np.linspace(-1, 7, 1000)\n",
    "y_range = f(x_range)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot function\n",
    "plt.plot(x_range, y_range, 'b-', linewidth=2, label='f(x) = (x-3)¬≤ + 5')\n",
    "\n",
    "# Plot gradient descent path\n",
    "history_y = [f(x) for x in history]\n",
    "plt.plot(history, history_y, 'ro-', markersize=8, linewidth=2, \n",
    "         label=f'GD Path ({len(history)} steps)')\n",
    "\n",
    "# Mark minimum\n",
    "plt.plot(3, 5, 'g*', markersize=20, label='True Minimum')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Gradient Descent Optimization', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gradient descent visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Learning Rate Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.1]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    try:\n",
    "        x_opt, history = gradient_descent(f, x0=0.0, learning_rate=lr, \n",
    "                                          n_iterations=50)\n",
    "        \n",
    "        # Plot\n",
    "        history_y = [f(x) for x in history]\n",
    "        plt.plot(x_range, y_range, 'b-', alpha=0.3, linewidth=2)\n",
    "        plt.plot(history, history_y, 'o-', color=colors[i], \n",
    "                 markersize=6, linewidth=2)\n",
    "        plt.plot(3, 5, 'g*', markersize=15)\n",
    "        \n",
    "        title = f'Learning Rate = {lr}\\nConverged: x={x_opt:.3f}'\n",
    "        if lr >= 1.0:\n",
    "            title += ' (Diverging!)'\n",
    "        \n",
    "        plt.title(title, fontweight='bold')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "    except:\n",
    "        plt.text(0.5, 0.5, 'Diverged!', ha='center', va='center', \n",
    "                 transform=plt.gca().transAxes, fontsize=20, color='red')\n",
    "        plt.title(f'Learning Rate = {lr} (FAILED)', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Effect of Learning Rate on Convergence', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"‚Ä¢ Too small (0.01): Slow convergence\")\n",
    "print(\"‚Ä¢ Just right (0.1-0.5): Fast, stable convergence\")\n",
    "print(\"‚Ä¢ Too large (1.1): Diverges!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 2D Optimization\n",
    "\n",
    "Let's optimize a function of two variables: $f(x, y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_2d(f, x0, y0, learning_rate=0.1, n_iterations=50):\n",
    "    \"\"\"Gradient descent for 2D functions.\"\"\"\n",
    "    x, y = x0, y0\n",
    "    history = [(x, y)]\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Partial derivatives\n",
    "        df_dx = numerical_derivative(lambda x: f(x, y), x)\n",
    "        df_dy = numerical_derivative(lambda y: f(x, y), y)\n",
    "        \n",
    "        # Update\n",
    "        x = x - learning_rate * df_dx\n",
    "        y = y - learning_rate * df_dy\n",
    "        history.append((x, y))\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Optimize f(x,y) = x¬≤ + y¬≤\n",
    "f_2d = lambda x, y: x**2 + y**2\n",
    "history_2d = gradient_descent_2d(f_2d, x0=3.0, y0=3.0, learning_rate=0.1)\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "X = np.linspace(-4, 4, 50)\n",
    "Y = np.linspace(-4, 4, 50)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "ax1.plot_surface(X, Y, Z, alpha=0.3, cmap='viridis')\n",
    "history_z = [f_2d(x, y) for x, y in history_2d]\n",
    "ax1.plot([p[0] for p in history_2d], \n",
    "         [p[1] for p in history_2d], \n",
    "         history_z, 'r.-', markersize=10, linewidth=2)\n",
    "ax1.set_title('3D Optimization Path', fontweight='bold')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "plt.colorbar(contour, ax=ax2)\n",
    "ax2.plot([p[0] for p in history_2d], [p[1] for p in history_2d], \n",
    "         'r.-', markersize=8, linewidth=2)\n",
    "ax2.plot(0, 0, 'g*', markersize=20, label='Minimum')\n",
    "ax2.set_title('Contour Plot with GD Path', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal position: ({history_2d[-1][0]:.6f}, {history_2d[-1][1]:.6f})\")\n",
    "print(f\"Expected: (0, 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Track Your Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from utils.progress_tracker import get_tracker\n",
    "\n",
    "tracker = get_tracker()\n",
    "tracker.mark_lesson_complete('module_1', 'project_1_3_optimization_basics', xp_earned=60)\n",
    "\n",
    "print(\"‚úì PROJECT 1.3 COMPLETED!\")\n",
    "\n",
    "stats = tracker.get_stats()\n",
    "print(f\"üèÜ XP: {stats['user']['total_xp']}\")\n",
    "print(f\"üìä Level: {stats['user']['current_level']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}